<!DOCTYPE html>
<html lang='en'>
  <head>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-172959135-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-172959135-1');
</script>

    <meta name="google-site-verification" content="gLhqpbQijxKMKurx90GylIsfVCXTRqhPTc7gIHsDD4s" />
    <meta charset='UTF-8'/>
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Projects</title>

    <link rel='stylesheet' href='./css/style.css'/>
    <link href="https://fonts.googleapis.com/css?family=Merriweather+Sans" rel="stylesheet">
  </head>

  <body>
      <nav>
        <div class="info">
          <div class="name"><a href="./">Jesse Murray</a></div>
          <aside></aside>
        </div>
        <div class="menu">
          <ul>


                <li>

                  <a href="./">Projects</a>

                </li>


                <li>

                  <a href="./about/">About</a>

                </li>



                <li>

                  <a href="./CV/">CV</a>

                </li>


                <li>

                  <a href="./music/">Music</a>

                </li>



                 <li>

                  <a href="./contact/">Contact</a>

                </li>


          </ul>
        </div>
        <div class="social">
            <ul>
            <li><a href=mailto:jessebmurray1@gmail.com><img alt="Email" width="20" height="20" src="./icons/mail.svg"></a></li>
            <li><a href="https://github.com/jessebmurray"><img alt="Github" width="20" height="20" src="./icons/github.svg"></a></li>
            </ul>
        </div>
      </nav>

      <article>
  <header>
    <h2><a href="./">Projects</a></h2>
    <aside>
The titles link to the project outcome and the dates indicate when I was most productive on the project.

    </aside>
  </header>

  <div class="content">
    <ul class="post-list">





<li>
          <h2><a class="post-link" href="project_files/JM_NASA_Poster.pdf">
    Algorithmic detection of elemental biosignatures
            </a></h2> <aside class="post-meta"> · Jul 2020</aside>
          <div class="third">
            <div>

              <img src="./images/poter_pca_1_2_3.png">

            </div>
            <div>
            <span class="blurb">

In this research, I build machine learning models that classify samples as indicative or non-indicative of life from chemical data. Such models can <a href="https://www.liebertpub.com/doi/full/10.1089/ast.2017.1773">support future life-detection missions</a> by suggesting the distinguishing properties of life, specifically how combinations of measurements can produce a better prediction than any singular measurement could. I have been working on this research under the mentorship of Diana Gentry for my summer internship at <a href="https://en.wikipedia.org/wiki/Ames_Research_Center">NASA Ames Research Center</a>. This (simplified) poster is for a web presentation I gave to NASA Ames. I will be presenting this research at the <a href="https://www.agu.org/fall-meeting">2020 American Geophysical Union Fall Meeting</a>.

            </span>
            </div>
          </div>
</li>








<li>
          <h2><a class="post-link" href="project_files/eyetracking_poster.pdf">Gaze sequences reveal how people gradually arrive at a solution to a word puzzle (anagram)</a></h2> <aside class="post-meta"> · Jul 2019</aside>
          <div class="third">
            <div>

              <img src="./images/img8.png">

            </div>
            <div>
            <span class="blurb">
In this research, I obtained new results about how the first and last two-letter bigrams of a solution word are highly informative of the solution to an anagram, as compared to the interior bigrams. This result has deep connections to a strongly supported result from reading studies that <a href="https://www.researchgate.net/publication/232609640_The_importance_of_first_and_last_letter_in_words_during_sentence_reading">
the first and last letters of words are more informative than the interior letters</a>. Our result from eye-tracking data provides an independent line of evidence in agreement with these results, and suggests an overall theory that internal word representations are weighted towards their beginnings and ends. In this year-long research project, I designed, ran, and analyzed the data from an eye-tracking experiment with 29 participants. I eventually formed and am continuing to lead a research team supervised by <a href="https://www.linkedin.com/in/minjoon-kouh-093751/">Minjoon Kouh</a>. I presented this research at the annual Brain and Behavior Conference at the University of Scranton in March 2020.



            </span>
            </div>
          </div>
</li>










<li>
          <h2><a class="post-link" href="project_files/polygenic_paper.pdf">A population model of polygenic inheritance</a></h2> <aside class="post-meta"> · Mar 2020</aside>
          <div class="third">
            <div>

              <img alt="" src="./images/individual_offspring_n1000_r50_r_s90_.png">

            </div>
            <div>
            <span class="blurb">
What follows is a non-technical abstract (the technical abstract is in the paper). Are the tallest members of a population mostly the children of tall couples or of average/shorter couples? You might expect most of the tall children to be from tall couples, because tall couples are more likely to have tall children. On the other hand, you remember that there are many more non-tall couples than tall couples, so by their superior number, non-tall couples might produce more tall children overall. So which is it? Does the greater likelihood overcome the greater number, or vice versa? The goal of this project was to derive a compuational model that quantified these effects and answered such questions precisely. What did it find? Well, if 'tall' is to be considered as within the top 10%, then the shorter couples win by far: only 30% of the tall members of a population are from tall parents. This model ended up also having a lot to say about intergenerational mobility and the  parameter values that allow for a stable population over time. The model made verifiable predictions that agreed remarkably well with <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1">Galton’s historic height dataset</a> (R^2 of 0.81) and with <a href="https://www.equality-of-opportunity.org/assets/documents/mobility_geo.pdf">US family income data</a> (R^2 of 0.96).




<!--

Consider the tallest trees in a forest. Are most of them the offspring of the last generation's
tallest or shorter/average trees?

Although tall trees have a higher probability of having tall offspring,
there are many more average/short trees than tall trees.

To answer the question, I created a model of normal population distributions reproducing with regression towards the mean.

The model was based on the                  <a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean#Definition_for_simple_linear_regression_of_data_points">
linear regression</a> equation of           <a href="https://www.nature.com/articles/pr1998502">
polygenic inheritance</a>.

It assumed small normal distributions about the
expected-offspring-value from each parent member in the previous
generation that sum to form the current generation distribution.

A simulation of the model found that past the 72nd percentile, the tallest trees are
mostly the offspring of shorter/average parents.

The model was also applied to intergenerational movement between quintiles and obtained
an r<sup>^2</sup> of 0.92 and 0.93 (p < .01) with  Brookings Institution measures of                  <a href="https://www.brookings.edu/blog/social-mobility-memos/2014/10/27/the-inheritance-of-education/">
intergenerational education and income mobility</a>, respectively.

The high correlations may have resulted from parent values in those distributions producing sub-offspring
distributions that regress towards the mean.
-->

            </span>
            </div>
          </div>
</li>









<li>
          <h2><a class="post-link" href="https://github.com/jessebmurray/COVID-19/blob/master/README.md">
         Using machine learning to predict COVID cases within each US county from the 2018 US Census Estimates data, and visualizations of the evolving relationship between the prevalence of COVID and population density
           </a></h2> <aside class="post-meta"> · May 2020</aside>
          <div class="third">
            <div>

              <img src="./images/xgb_performance.png">

            </div>
            <div>
            <span class="blurb">

In this project, I applied gradient boosted decision trees to 2018 US Census Estimates data to predict the number of confirmed COVID-19 cases per 100,000 within each US county.
I then identified the counties with the biggest negative residuals, in other words, the counties for which the model most over-predicted the prevalence of COVID-19.
These counties can be considered the 'luckiest' counties, as they have fared much better than was predicted from their US census data.
Or perhaps, testing may need to be expanded in these counties, as they may have far more cases than presently reported.

I also explore how the population density of a US county can be used to predict its prevalence of COVID-19, and how this relationship changes as the virus continues its march across urban, suburban, and rural counties alike.
Lastly, I explore the relative changes in people's interests during the pandemic lockdown by looking at the changes in various google searches at the (previous) US epicenter of the crisis: NYC.
The case data in this project is from May 16, 2020.


            </span>
            </div>
          </div>
</li>

















        <li>
          <h2><a class="post-link" href="https://github.com/jessebmurray/tuning/blob/master/tuning.ipynb">Explorations of a proposed tuning system with comparisons to Pythagorean and twelve-tone equal temperament tuning</a></h2> <aside class="post-meta"> · Feb 2020</aside>
          <div class="third">
            <div>

              <img src="./images/img7.png">

            </div>
            <div>
            <span class="blurb">

I demonstrate a tuning system I thought of, which is compared with the well known pythagorean tuning method.
These two methods are shown to be derived with a few algebraic rules.
The two algorithms are compared in terms of their simplicity, correspondance to the consonance of intervals, and errors with the well-established twelve tone equal temperament tuning method (how out of tune they sound).
The proposed inverse fraction rule is found to have a surprisingly low error rate with the twelve tone inverse.
Here's the proposed tuning algorithm: Begin with the fraction 2/1. Multiply the inverse of the fraction by 2 to get the inverse of the interval.
Add 1 to the top and bottom of the fraction to get the next interval. Etc.
The proposed tuning system has some interesting properties.
For example, the order of intervals in the algorithm corresponds to the <a href="https://www.stat.yale.edu/~zf59/MathematicsOfMusic.pdf">consonance</a> of the intervals.
On the other hand, the tuning system does not include the tritone and has a larger average absolute error with twelve-tone equal temperament than Pythagorean tuning has (1.91% versus 0.258%).
Probably the most important discovery is the inverse fraction rule, which locates a close approximation of the twelve-tone inverse from any interval fraction.
I'm currently working on exploring the intriguing features of the errors for this rule.

            </span>
            </div>
          </div>
        </li>


        <li>
          <h2><a class="post-link" href="https://github.com/jessebmurray/landfills/blob/master/landfills_explainer.ipynb">Simulating the amount of non-decomposed municipal solid waste material in US landfills into the future</a></h2> <aside class="post-meta"> · Nov 2019</aside>
          <div class="third">
            <div>

              <img src="./images/landfill.png">

            </div>
            <div>
            <span class="blurb">

                I simulate the growth in the amount of non-decomposed material in US landfills. I use an exponential decay model of decomposition and literature-provided parameter values for decomposition times, MSW material densities, MSW material historical and projected annual landfill additions, and US landfill depths.
              Main results: By the year 2100, plastic is the largest contributor to total US landfill size in terms of mass and the majority contributor in terms of volume and land area. However, thousands of years into the future, glass becomes the majority contributor due to its 1,000,000-year decomposition time. The amount of non-decomposed material is projected to take up about 400 sq km of US land area by 2100, and about 33,000 sq km (roughly the size of Massachusetts) at equilibrium (when rate of decomposition equals rate of addition). If the US stops landfilling glass in the year 2200, the equilibrium land area is shown to be reduced to about half the size of Rhode Island.

            </span>
            </div>
          </div>
        </li>



        <li>

          <h2><a class="post-link" href="https://github.com/jessebmurray/leverage/blob/master/README.md">An analysis of the returns from simulated leveraged index funds</a></h2> <aside class="post-meta"> · Aug 2017</aside>
          <div class="third">
            <div>

              <img src="./images/leverage-10yr-relative-percentiles.png">

            </div>
            <div>
            <span class="blurb">


Leveraged ETFs have recently become well known investment instruments. Examples include UPRO and TQQQ, which return 3x the daily return of the S&P 500 and Nasdaq 100, respectively. That is, if the S&P 500 falls by 1% in a single trading day, UPRO will fall by 3% - and vice versa if the S&P 500 rises in price. In 2017, I became interested in how leveraged index funds could be used over the long-term, and so I created some <a href="https://drive.google.com/file/d/17ZEjg1gfVvGroazf7V_1WRU2w1egh_qS/view?usp=sharing">enormous excel sheets</a> and wrote up a little <a href="project_files/leveraged_index.pdf">report on my findings</a>. More recently, I returned to the problem and wrote highly efficient and concise <a href="https://numpy.org/">NumPy</a> code to get the job done. I combined separate historical S&P 500 and annual dividend yield data going back to 1928. I looked at the performance of investing x% of an equity portfolio in a leveraged index, with the remaining in the standard, non-leveraged index, assuming reinvestment of dividends and the tax-advantaged strategy of rebalancing the portfolio (returning to the x%) every year. I analyze the performance of this strategy with leverage rates from 0% to 100% across all 20,756 possible ten-year periods since 1928. I also made a copy of the notebook that looks at all twenty-year periods. What I found was that leveraging is certainly a risk and should therefore be a function of proximity-to-retirement. However, small amounts of leverage appear to be able to boost equity returns over the long-term.

<!--
I simulate the S&P 500 with annual rebalancing of leveraging rates from 0% to 100% across 1,144 historical 10-year periods each spaced one week apart. The index funds used are: The NASDAQ Composite, the S&P 500, and the Dow Jones Industrial Average. Main results: All indices had higher median returns with increased leverage, but there were diminishing and even decreasing median returns as leverage increased past 50%. For example: at 50% leverage, the S&P 500 had a median 10-year compound interest rate of 12% as compared to 8.3% for unleveraged. Even in the worst 20-year period (out of 624), moderate amounts of leverage (up to 40%) had higher compound interest rates for all three indices.
-->

            </span>
            </div>
          </div>
        </li>


        <li>
          <h2><a class="post-link" href="https://github.com/jessebmurray/child_mortality_fertility_rates/blob/master/child_mortality_fertility_rates_explainer.ipynb">The probabilistic implications of the high pre-1800 global child mortality and fertility rates</a></h2> <aside class="post-meta"> · June 2019</aside>
          <div class="third">
            <div>

              <img src="./images/prob_rep_success.png">

            </div>
            <div>
            <span class="blurb">

I lend support to the <a href="https://www.ncbi.nlm.nih.gov/books/NBK233807/">theory</a> that the very high pre-1800 global fertility rate was a response to the very high global child mortality rate. I use probabilities from a binomial distribution to show how the desire for parents to have at least two of their children survive into adulthood could have led to a strategy of having many children, among other factors. Main results: Having 3 children instead of 2 doubled the probability that at least 2 survive past five years old (from 32% to 60%). When having 6 children (the pre-1800 average), parents obtained a 94% chance of reproductive success, i.e., that at least 2 survive past childhood. It is postulated that around this number of children, the reproductive benefits of having an additional (seventh) child would be outweighed by the costs.


            </span>
            </div>
          </div>
        </li>


        <li>
          <h2><a class="post-link" href="project_files/college_majors.pdf">The relative competitiveness and economic rewards of college majors</a></h2> <aside class="post-meta"> · Mar 2019</aside>
          <div class="third">
            <div>

              <img src="./images/img4.png">

            </div>
            <div>
            <span class="blurb">
I compare the relative competitiveness and economic rewards (income) of different college majors using SAT score and income data. Main results: The English, Language, and Philosophy/religious majors, i.e., humanities majors, are both very competitive as well as very economically unrewarding. On the other hand, the Physical science and Biology majors are highly competitive and relatively economically unrewarding, that is, until a graduate degree is obtained. Then, those majors become very economically rewarding. For overall score - in terms of getting the most economic reward for the least competitiveness - the 'best' undergraduate degrees are Engineering and Computer Science.

<!--
I compare the relative competitiveness and economic rewards of different college majors both for bachelor's degree only as well as graduate degree holders using SAT score and income data. Main results: The English, Language, and Philosophy/religious are both highly competitive and highly economically unrewarding. Physical science and Biology majors are highly competitive and relatively economically unrewarding, until a graduate degree is obtained. Then those majors become highly economically rewarding. For overall score - in terms of maximizing the economic rewards as well as the ratio of the rewards to the competitiveness - if one only obtains a bachelor's degree the 'best' majors are Engineering and Computer Science.
-->
            </span>
            </div>
          </div>
        </li>



        <li>
          <h2><a class="post-link" href="https://docs.google.com/spreadsheets/d/1ay8tjj6jVbaNAdIJvGYLraRV3cE13I63lFXYeF3J29g/edit?usp=sharing">Time travel with special relativity (general hopefully coming soon)</a></h2> <aside class="post-meta"> · Jul 2019</aside>
          <div class="third">
            <div>
                  <img src="./images/img2.png">
            </div>
            <div>
            <span class="blurb">

Is time travel technically possible? Well, yes! Although, you can only travel forward in time, and the energy requirements are rather ridiculous. How do you do it? You go fast, very fast. In doing so, you take advantage of the remarkable fact that light moves away from all observers at the same speed. That is, if someone drives past you on a motorcycle going 90% the speed of light and turns on a flashlight, the light from the flashlight will go away from you <i> and him </i> at the same speed. This seemingly impossible fact leads mathematically to all sorts of wondrous effects: from the Earth shrinking to the width of a pancake, to seeing radio waves. (Actually there is one other way to time travel, which is to get next to something heavy, but I haven't covered that yet.) In this project, I thought it would be cool to create a virtual time machine. Simply enter the date in the future you'd like to go to and how fast you'd like to travel, then see all the amazing things that will happen during your journey, like how much fuel you'll need for your spaceship (probably a lot). I made this when I was the TA for a special relativity class at the <a href="https://www.drew.edu/governors-school/">New Jersey Governor’s School in the Sciences</a>. I hoped to show how the significant energy requirements of time travel make it practically out of reach for the foreseeable future. I hope to come back to this project eventually and make a fun web application.
            </span>
            </div>
          </div>
        </li>






        <li>
          <h2><a class="post-link" href="project_files/nutrition.pdf">Comparing foods with calculations on nutrition facts data</a></h2> <aside class="post-meta"> · Aug 2018</aside>
          <div class="third">
            <div>

              <img src="./images/img6.png">

            </div>
            <div>
            <span class="blurb">

                I compare foods in terms of overall healthiness score, protein content, energy density, cost, and others. I standardize comparisons across diverse food types by using energy measurements, i.e., the contents of a day's worth (~2000 calories). The data I use are price, mass, energy, total fat, saturated fat, polyunsaturated fat, monounsaturated fat, trans fat, carbohydrates, fiber, protein, and glycemic index.

            </span>
            </div>
          </div>
        </li>



        <li>
          <h2><a class="post-link" href="project_files/climate_change.pdf">Climate change and the science behind emissions</a></h2> <aside class="post-meta"> · Feb 2020</aside>
          <div class="third">
            <div>

              <img src="./images/world.png">

            </div>
            <div>
            <span class="blurb">

Thinking and reading about climate change has long been a personal hobby of mine. I am actually a contributor to the <a href="https://en.wikipedia.org/wiki/climate_change">Wikipedia page on climate change</a>. My Wikipedia contributions were recently noticed by a journalist at Mashable, and I was interviewed for an article that will be coming out in November. These are the slides of a 30-minute talk I gave for a student club, in which I discussed the many diverse sources of anthropogenic greenhouse gas emissions and how we might go about eliminating those emissions.
The talk came from my passion for multi-disciplinary science, and it was created entirely in my free time, purely out of my deep interest in the topic. Humanity is faced with the multi-dimensional scientific, geopolitical, and engineering problem of how to reduce global greenhouse gas emissions from the current 50 billion metric tons per year (and rising) <a href="https://www.ipcc.ch/sr15/chapter/spm/spm-c/spm3a/">to nearly zero - all within the next few decades</a>. At first glance, the solution appears to come from merely changing <a href="https://kleinmanenergy.upenn.edu/sites/default/files/Getting_to_Zero.pdf">energy sourcing and storage</a>.

However, as I demonstrate in the lecture, tackling climate change is much more complex. Firstly, 81% of global energy consumption is not provided by electricity grids, and therefore would not be renewable even if solar or wind were fully deployed today. As a result, we are faced with having to cost-effectively provide that pesky 81% (e.g., firing cement kilns, cross-ocean transport, indoor heating, etc.) from either electricity grids, battery storage, or hydrogen fuel. Some common themes arise from this challenge including the extremely high energy density of petroleum and the widespread use of flame heating in manufacturing. Secondly, a substantial amount of greenhouse gas is emitted from the <i>chemical reactions</i> for making basic materials that are currently deeply embedded in modern civilization: steel (iron smelting), cement (calcination), aluminum (Hall-Héroult process), fertilizer (Haber-Bosch process), plastic (steam-cracking), and paper (Kraft process). Lastly, a challenging plethora of small diverse sources remain, incuding the methane emitted by decomposing organics in landfills, the nitrous oxide emitted by the microbial decomposition of fertilizer, the methane emitted by rice paddies, the CO<sub>2</sub> emitted by the gradual degradation of soil, etc. I argue that substantial research and development funding is needed as early as possible in order to create cost-effective and non-emitting alternatives to the many sources of greenhouse gas emissions.

            </span>
            </div>
          </div>
        </li>




    </ul>
  </div>

  <footer>
    <aside>




I have some project-ideas still on my might-do/unexplored list, they are on topics including:

<p>

    <li> A cost-benefit model of speeding in relation to the ticket price and the probability of getting pulled over. </li>

  <li>
    Training a model to recognize the features of better vs. worse melodies, checking the model against the characteristics of good
  melody as proposed by <a href="https://www.juilliard.edu/music/faculty/laitz-steven"><u>Steven Laitz</u></a>.
  </li>

    <li> A geographic visualization of weather data through the feels-like scale, the seasonal chance of pretty/ugly weather events such as snow
  and rainfall, and the proportion of time spent below freezing point.
  </li>

<li>
    The fastest walking path through the Manhattan grid based on start and end locations, taking into account waiting times at crosswalks; i.e., it is not sides a and b of a right triangle.
</li>

<!--
<li>
    Listing the non-proper-noun words of the most common words in English and grouping together those that are synonyms
  of one another to obtain the number of unique semantic representations as compared to the number of unique words. Playing around with
  the resulting data on words per unique semantic representation.
</li>
-->


<li>
    Using data on people's music listening habits to better understand the pleasantness of a song as a function of the number
  of listens. I believe meaningful results could be obtained with even a relatively small sample size of ~50.
</li>

</p>

Feel free to <b><a href="./contact/">contact me</a></b> :)

<br><br>

Credits for the CSS of this website go entirely to <a href="https://aatishb.com/"><b>Aatish Bhatia</b></a>, who has done some very interesting and important research.



<!-- Leave some space at the bottom (for iPhone)-->
<br>
<br>



<!--
<br>
<br>

The CSS for this website was drawn from the website of someone whose work I greatly admire: <a href="https://aatishb.com/"><u>Aatish Bhatia</u></a>.
-->

    </aside>
  </footer>
</article>



  <script data-cfasync="false" src="./cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script></body>

</html>
