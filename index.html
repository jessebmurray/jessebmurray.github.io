<!DOCTYPE html>
<html lang='en'>
  <head>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-172959135-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-172959135-1');
</script>

    <meta name="google-site-verification" content="gLhqpbQijxKMKurx90GylIsfVCXTRqhPTc7gIHsDD4s" />
    <meta charset='UTF-8'/>
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Projects</title>

    <link rel='stylesheet' href='./css/style.css'/>
    <link href="https://fonts.googleapis.com/css?family=Merriweather+Sans" rel="stylesheet">
  </head>

  <body>
      <nav>
        <div class="info">
          <div class="name"><a href="./">Jesse Murray</a></div>
          <aside></aside>
        </div>
        <div class="menu">
          <ul>


                <li>

                  <a href="./">Projects</a>

                </li>


                <li>

                  <a href="./about/">About</a>

                </li>



                <li>

                  <a href="./CV/">CV</a>

                </li>


                <li>

                  <a href="./music/">Music</a>

                </li>



                 <li>

                  <a href="./contact/">Contact</a>

                </li>


          </ul>
        </div>
        <div class="social">
            <ul>
            <li><a href=mailto:jessebmurray1@gmail.com><img alt="Email" width="20" height="20" src="./icons/mail.svg"></a></li>
            <li><a href="https://github.com/jessebmurray"><img alt="Github" width="20" height="20" src="./icons/github.svg"></a></li>
            </ul>
        </div>
      </nav>

      <article>
  <header>
    <h2><a href="./">Projects</a></h2>
    <aside>
The titles link to the project outcome and the dates indicate when I was most productive on the project.

    </aside>
  </header>

  <div class="content">
    <ul class="post-list">





<li>
          <h2><a class="post-link" href="project_files/JM_NASA_Poster.pdf">
    Algorithmic detection of elemental biosignatures
            </a></h2> <aside class="post-meta"> · Jul 2020</aside>
          <div class="third">
            <div>

              <img src="./images/poter_pca_1_2_3.png">

            </div>
            <div>
            <span class="blurb">

In this research, I build machine learning models that classify samples as indicative or non-indicative of life from chemical data. Such models can <a href="https://www.liebertpub.com/doi/full/10.1089/ast.2017.1773">support future life-detection missions</a> by suggesting the distinguishing properties of life, specifically how combinations of measurements can produce a better prediction than any singular measurement could. I have been working on this research under the mentorship of Diana Gentry for my summer internship at <a href="https://en.wikipedia.org/wiki/Ames_Research_Center">NASA Ames Research Center</a>. This (simplified) poster is for a web presentation I gave to NASA Ames. I will be presenting this research at the <a href="https://www.agu.org/fall-meeting">2020 American Geophysical Union Fall Meeting</a>.

            </span>
            </div>
          </div>
</li>








<li>
          <h2><a class="post-link" href="project_files/eyetracking_poster.pdf">Gaze sequences reveal how people gradually arrive at a solution to a word puzzle (anagram)</a></h2> <aside class="post-meta"> · Jul 2019</aside>
          <div class="third">
            <div>

              <img src="./images/img8.png">

            </div>
            <div>
            <span class="blurb">

In this research, I obtained new findings about the mostly unconscious cognitive process that occurs when solving an <a href="https://www.gamesforthebrain.com/game/anagramania/">anagram puzzle</a>. Specifically, I found that the beginnings and ends of solution words are significantly more informative than their middles. This result has deep connections to a strongly supported result from reading studies that <a href="https://www.researchgate.net/publication/232609640_The_importance_of_first_and_last_letter_in_words_during_sentence_reading">
the first and last letters of words are more informative than the interior letters</a>. The similar finding from anagram problem-solving provides an independent line of evidence in agreement with these results, pointing to an overall theory that internal word representations are weighted towards their beginnings and ends. Over the course of a year, I designed and oversaw an original eye-tracking experiment of 29 participants. I wrote code to gain insights from the messy coordinate-time-series data (90 million rows), and eventually I formed and led a small research team of students supervised by <a href="https://www.linkedin.com/in/minjoon-kouh-093751/">Minjoon Kouh</a>. In March 2020, I presented the results of the research at the annual Brain and Behavior Conference at the University of Scranton.

            </span>
            </div>
          </div>
</li>










<li>
          <h2><a class="post-link" href="project_files/polygenic_paper.pdf">A population model of polygenic inheritance</a></h2> <aside class="post-meta"> · Mar 2020</aside>
          <div class="third">
            <div>

              <img alt="" src="./images/individual_offspring_n1000_r50_r_s90_.png">

            </div>
            <div>
            <span class="blurb">
What follows is a non-technical abstract (the technical abstract is in the paper). Are the tallest members of a population mostly the children of tall couples or of average/shorter couples? You might expect most of the tall children to be from tall couples, because tall couples are more likely to have tall children. On the other hand, you remember that there are many more non-tall couples than tall couples, so by their superior number, non-tall couples might produce more tall children overall. So which is it? Does the greater likelihood overcome the greater number, or vice versa? The goal of this project was to derive a computational model that quantified these effects and answered such questions precisely. What did it find? Well, if 'tall' is to be considered as within the top 10%, then the shorter couples win by far: only 30% of the tall members of a population are from tall parents. This model ended up also having a lot to say about intergenerational mobility and the  parameter values that allow for a stable population over time. The model made verifiable predictions that agreed remarkably well with <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1">Galton’s historic height dataset</a> (R^2 of 0.81) and with <a href="https://www.equality-of-opportunity.org/assets/documents/mobility_geo.pdf">US family income data</a> (R^2 of 0.96).



            </span>
            </div>
          </div>
</li>









<li>
          <h2><a class="post-link" href="https://github.com/jessebmurray/COVID-19/blob/master/README.md">
         Using machine learning to predict COVID cases within each US county from the 2018 US Census Estimates data, and visualizations of the evolving relationship between the prevalence of COVID and population density
           </a></h2> <aside class="post-meta"> · May 2020</aside>
          <div class="third">
            <div>

              <img src="./images/xgb_performance.png">

            </div>
            <div>
            <span class="blurb">

In this project, I applied gradient boosted decision trees to 2018 US Census Estimates data to predict the number of confirmed COVID-19 cases per 100,000 within each US county.
I then identified the counties with the biggest negative residuals, in other words, the counties for which the model most over-predicted the prevalence of COVID-19.
These counties can be considered the 'luckiest' counties, as they have fared much better than was predicted from their US census data.
Or perhaps, testing may need to be expanded in these counties, as they may have far more cases than presently reported.

I also explore how the population density of a US county can be used to predict its prevalence of COVID-19, and how this relationship changes as the virus continues its march across urban, suburban, and rural counties alike.
Lastly, I explore the relative changes in people's interests during the pandemic lockdown by looking at the changes in various google searches at the (previous) US epicenter of the crisis: NYC.
The case data in this project is from May 16, 2020.


            </span>
            </div>
          </div>
</li>

















        <li>
          <h2><a class="post-link" href="./tuning">Explorations of a proposed tuning system with comparisons to Pythagorean and twelve-tone equal temperament tuning</a></h2> <aside class="post-meta"> · Feb 2020</aside>
          <div class="third">
            <div>

              <img src="./images/img7.png">

            </div>
            <div>
            <span class="blurb">

I demonstrate a tuning system I thought of, which is compared with the well known pythagorean tuning method.
These two systems are shown to essentially be algorithms that produce rational fractions from simple repeating algebraic rules.
The two algorithms are compared in terms of their simplicity, correspondence to the consonance of intervals, and errors with the well-established twelve-tone equal temperament tuning method (how out of tune they sound).
Here's the proposed tuning algorithm: Begin with the interval fraction 2/1. Multiply the inverse of the fraction by 2 to get the musical inverse.
Add 1 to the top and bottom of the fraction to get the next interval. Repeat these two steps again and again.
The proposed tuning system has some interesting properties.
For example, the order of intervals in the algorithm corresponds to the <a href="http://www.stat.yale.edu/~zf59/MathematicsOfMusic.pdf">consonance</a> of intervals. The proposed algorithm also has a continuous and unbroken pattern, whereas the Pythagorean algorithm has one break in the pattern. On the other hand, the tuning system does not include the tritone and has a larger average absolute error with twelve-tone equal temperament than Pythagorean tuning has (1.91% versus 0.258%). A visualization of the tuning algorithms is presented.

            </span>
            </div>
          </div>
        </li>


        <li>
          <h2><a class="post-link" href="./landfills">Modeling the growth of non-decomposed municipal solid waste in US landfills</a></h2> <aside class="post-meta"> · Nov 2019</aside>
          <div class="third">
            <div>

              <img src="./images/landfill.png">

            </div>
            <div>
            <span class="blurb">

I use an exponential decay model of decomposition and literature-provided parameter values for decomposition times, MSW material densities, MSW material historical and projected annual landfill additions, and US landfill depths.
Main results: By the year 2100, plastic is the largest contributor to total US landfill size in terms of mass and the majority contributor in terms of volume and land area. However, thousands of years into the future, glass becomes the majority contributor due to its roughly estimated 1,000,000-year decomposition time. The amount of non-decomposed material is projected to take up about 400 sq km of US land area by 2100, and about 33,000 sq km (roughly the size of Massachusetts) at equilibrium (when rate of decomposition equals rate of addition). If the US stops landfilling glass in the year 2200, the equilibrium land area is shown to be reduced to about half the size of Rhode Island. This was my first real computational modeling project and I learned so much about best practices, quality code, etc.

            </span>
            </div>
          </div>
        </li>



        <li>

          <h2><a class="post-link" href="./leverage">An analysis of the returns from simulated leveraged index funds</a></h2> <aside class="post-meta"> · Aug 2017</aside>
          <div class="third">
            <div>

              <img src="./images/leverage-10yr-relative-percentiles.png">

            </div>
            <div>
            <span class="blurb">


Leveraged ETFs have been gaining attention as high-risk investment instruments that can potentially provide large multiples of the underlying indices they track. One prominent example is UPRO, which returns 3x the daily return of the S&P 500. That is, if the S&P 500 falls by 1% in a single trading day, UPRO will fall by 3% - and vice versa if the S&P 500 rises in price. In 2017, I became interested in how leveraged index funds could be used over the long-term, and so I created some <a href="https://drive.google.com/file/d/17ZEjg1gfVvGroazf7V_1WRU2w1egh_qS/view?usp=sharing">enormous excel sheets</a> and wrote up a little <a href="project_files/leveraged_index.pdf">report on my findings</a>. More recently, I returned to the problem and wrote highly efficient and concise <a href="https://numpy.org/">NumPy</a> code to get the job done. I combined historical S&P 500 daily price and annual dividend yield data going back to 1927. I simulated the returns from investing a certain percentage of an equity portfolio in a leveraged index, and the remaining in the standard, non-leveraged index, assuming reinvestment of dividends and the tax-advantaged strategy of rebalancing the portfolio once a year (returning to the original leverage percentage). I analyze the performance of this strategy with leverage percentages from 0% to 100% across all 20,756 possible ten-year periods since 1927. What I found was that leveraging is indeed a substantial risk and should therefore, if used at all, be a function of proximity-to-retirement. However, small amounts of leverage appear to boost equity returns over the long-term.

            </span>
            </div>
          </div>
        </li>


        <li>
          <h2><a class="post-link" href="./cmf">The probabilistic implications of the high pre-1800 global child mortality and fertility rates</a></h2> <aside class="post-meta"> · June 2019</aside>
          <div class="third">
            <div>

              <img src="./images/prob_rep_success.png">

            </div>
            <div>
            <span class="blurb">

I lend support to the <a href="https://www.ncbi.nlm.nih.gov/books/NBK233807/">theory</a> that the very high pre-1800 global fertility rate was a response to the very high global child mortality rate. I use probabilities from a binomial distribution to show how the desire for parents to have at least two of their children survive into adulthood could have led to a strategy of having many children, among other factors. One limitation of the simple binomial approach however is that it assumes the probability of child mortality is independent between births. Main results: Having 3 children instead of 2 doubled the probability that at least 2 survive past five years old (from 32% to 60%). When having 6 children (the pre-1800 average), parents obtained a 94% chance of reproductive success, i.e., that at least 2 survive past childhood. It is postulated that around this number of children, the reproductive benefits of having an additional (seventh) child would be outweighed by the costs.


            </span>
            </div>
          </div>
        </li>


        <li>
          <h2><a class="post-link" href="project_files/college_majors.pdf">The relative competitiveness and economic rewards of college majors</a></h2> <aside class="post-meta"> · Mar 2019</aside>
          <div class="third">
            <div>

              <img src="./images/img4.png">

            </div>
            <div>
            <span class="blurb">
I compare the relative competitiveness and economic rewards of different college majors from SAT score and income data. Main results: The English, Language, and Philosophy/religious majors, i.e., humanities majors, are both very competitive as well as very economically unrewarding. On the other hand, the Physical science and Biology majors are highly competitive and relatively economically unrewarding, that is, until a graduate degree is obtained. Then, those majors become very economically rewarding. For overall score - in terms of getting the most economic reward for the least competitiveness - the 'best' undergraduate degrees are Engineering and Computer Science.


            </span>
            </div>
          </div>
        </li>



        <li>
          <h2><a class="post-link" href="https://docs.google.com/spreadsheets/d/1ay8tjj6jVbaNAdIJvGYLraRV3cE13I63lFXYeF3J29g/edit?usp=sharing">Time travel with special relativity (general hopefully coming soon)</a></h2> <aside class="post-meta"> · Jul 2019</aside>
          <div class="third">
            <div>
                  <img src="./images/img2.png">
            </div>
            <div>
            <span class="blurb">

Is time travel technically possible? Well, yes! Although, you can only travel forward in time, and the energy requirements are rather ridiculous. How do you do it? You go fast, very fast. In doing so, you take advantage of the remarkable fact that light moves away from all observers at the same speed. That is, if someone drives past you on a motorcycle going 90% the speed of light and turns on a flashlight, the light from the flashlight will go away from you <i>and the motorcyclist</i> at the same speed. This seemingly impossible fact leads mathematically to all sorts of wondrous effects: from the Earth shrinking to the width of a pancake, to seeing radio waves. (Actually there is one other way to time travel, which is to get next to something heavy, but I haven't covered that yet.) In this project, I thought it would be cool to create a virtual time machine. Simply enter a date in the future and how quickly you'd like to get there, then see all the amazing things that will happen during your journey, like how much fuel you'll need for your spaceship (a ridiculous amount). I made this when I was the TA for a special relativity class at the <a href="https://www.drew.edu/governors-school/">New Jersey Governor’s School in the Sciences</a>. I hoped to vividly demonstrate how the significant energy requirements of time travel make it practically out of reach for the foreseeable future. I hope to come back to this project eventually and make a fun web application.
            </span>
            </div>
          </div>
        </li>






        <li>
          <h2><a class="post-link" href="project_files/nutrition.pdf">Comparing foods with calculations on nutrition facts data</a></h2> <aside class="post-meta"> · Aug 2018</aside>
          <div class="third">
            <div>

              <img src="./images/img6.png">

            </div>
            <div>
            <span class="blurb">

    I compare foods in terms of overall healthiness score, protein content, energy density, cost, and others. I standardize comparisons across diverse food types by using a day's worth of energy requirements (~2000 calories). The data I use are price, mass, energy, total fat, saturated fat, polyunsaturated fat, monounsaturated fat, trans fat, carbohydrates, fiber, protein, and glycemic index.

            </span>
            </div>
          </div>
        </li>



        <li>
          <h2><a class="post-link" href="project_files/climate_change.pdf">Climate change and the science behind emissions</a></h2> <aside class="post-meta"> · Feb 2020</aside>
          <div class="third">
            <div>

              <img src="./images/world.png">

            </div>
            <div>
            <span class="blurb">

Thinking and reading about climate change has long been a personal hobby of mine. I am actually a contributor to the <a href="https://en.wikipedia.org/wiki/climate_change">Wikipedia page on climate change</a>. My Wikipedia contributions were recently noticed by a journalist at Mashable, and I was interviewed for an article that will be coming out in November. These are the slides of a 30-minute talk I gave for a student club, in which I discussed the many diverse sources of anthropogenic greenhouse gas emissions and how we might go about eliminating those emissions.
The talk came from my passion for multi-disciplinary science, and it was created entirely in my free time, purely out of my deep interest in the topic. Humanity is faced with the multi-dimensional scientific, geopolitical, and engineering problem of how to reduce global greenhouse gas emissions from the current 50 billion metric tons per year (and rising) <a href="https://www.ipcc.ch/sr15/chapter/spm/spm-c/spm3a/">to nearly zero - all within the next few decades</a>. At first glance, the solution appears to come from merely changing <a href="https://kleinmanenergy.upenn.edu/sites/default/files/Getting_to_Zero.pdf">energy sourcing and storage</a>.

However, as I demonstrate in the lecture, tackling climate change is much more complex. Firstly, 81% of global energy consumption is not provided by electricity grids, and therefore would not be renewable even if solar or wind were fully deployed today. As a result, we are faced with having to cost-effectively provide that pesky 81% (e.g., firing cement kilns, cross-ocean transport, indoor heating, etc.) from either electricity grids, battery storage, or hydrogen fuel. Some common themes arise from this challenge including the extremely high energy density of petroleum and the widespread use of flame heating in manufacturing. Secondly, a substantial amount of greenhouse gas is emitted from the <i>chemical reactions</i> for making basic materials that are currently deeply embedded in modern civilization: steel (iron smelting), cement (calcination), aluminum (Hall-Héroult process), fertilizer (Haber-Bosch process), plastic (steam-cracking), and paper (Kraft process). Lastly, a challenging plethora of small diverse sources remain, including the methane emitted by decomposing organics in landfills, the nitrous oxide emitted by the microbial decomposition of fertilizer, the methane emitted by rice paddies, the CO<sub>2</sub> emitted by the gradual degradation of soil, etc. I argue that substantial research and development funding is needed as early as possible in order to create cost-effective and non-emitting alternatives to the many sources of greenhouse gas emissions.

            </span>
            </div>
          </div>
        </li>




    </ul>
  </div>

  <footer>
    <aside>




I have some project-ideas still on my might-do/unexplored list, they are on topics including:

<p>

    <li> A cost-benefit model of speeding in relation to the ticket price and the probability of getting pulled over. </li>

  <li>
    Training a model to recognize the features of better vs. worse melodies, checking the model against the characteristics of good
  melody as proposed by <a href="https://www.juilliard.edu/music/faculty/laitz-steven"><u>Steven Laitz</u></a>.
  </li>

    <li> A geographic visualization of weather data through the feels-like scale, the seasonal chance of pretty/ugly weather events such as snow
  and rainfall, and the proportion of time spent below freezing point.
  </li>

<li>
    The fastest walking path through the Manhattan grid based on start and end locations, taking into account waiting times at crosswalks; i.e., it is not sides a and b of a right triangle.
</li>




<li>
    Using data on people's music listening habits to better understand the pleasantness of a song as a function of the number
  of listens. I believe meaningful results could be obtained with even a relatively small sample size of ~50.
</li>

</p>

Feel free to <b><a href="./contact/">contact me</a></b> :)

<br><br>

Credits for the CSS of this website go entirely to <a href="https://aatishb.com/"><b>Aatish Bhatia</b></a>, who has done some very interesting and important research.



<!-- Leave some space at the bottom (for iPhone)-->
<br>
<br>




    </aside>
  </footer>
</article>



  <script data-cfasync="false" src="./cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script></body>

</html>
