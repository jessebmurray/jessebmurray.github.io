<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">

<title>Modeling as a way of trying to be less surprised</title>

<link rel="stylesheet" href="LaTeXML.css" type="text/css">
<link rel="stylesheet" href="ltx-article.css" type="text/css">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<style>
    .content {
        max-width: 700px;
        margin: 0 auto;
        padding: 17px;
    }
    .ltx_equation.ltx_eqn_table {
        display: block;
        overflow-x: auto;
    }
    body {
        font-size: 18px;
    }
</style>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Modeling as a way of trying to be
<br class="ltx_break">less surprised</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jesse Murray
</span></span>
</div>

<div id="p1" class="ltx_para">
<p class="ltx_p">What does the word <em class="ltx_emph ltx_font_italic">probability</em> mean? One could argue that given the laws of physics and the initial conditions, all events are deterministic, so there is no such thing as probability. While we may know the laws of physics, we do not know the initial conditions of all particles in the Universe. Therefore, we do not know all of the relevant information that determines whether an event occurs or not. Due to this lack of complete information, we reason about the world under uncertainty and we quantify our uncertainty by assigning probabilities to events.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="images/surprise_painting.jpg" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="300" alt="Refer to caption"> <!-- height="1015"-->
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Sunday at the Museum, Honoré Daumier.</figcaption>
</figure>
<section id="S0.SS0.SSSx1" class="ltx_subsubsection">
<h2 class="ltx_title ltx_title_subsubsection">Two interpretations of probability</h2>

<div id="S0.SS0.SSSx1.p1" class="ltx_para">
<p class="ltx_p">There are two broad interpretations of the probability <math id="S0.SS0.SSSx1.p1.m1" class="ltx_Math" alttext="p\in(0,1)" display="inline"><mrow><mi>p</mi><mo>∈</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></math> of an event. (1) The <a href="https://en.wikipedia.org/wiki/Frequentist_probability" title="" class="ltx_ref ltx_href">frequentist interpretation</a> considers probability to be the relative frequency of an event’s occurrence as the situation in which the event could occur is repeated indefinitely. For example, if we consider flipping a weighted coin hundreds of times and observing it land heads about 42% of the time, we could estimate from this frequency that the coin has probability <math id="S0.SS0.SSSx1.p1.m2" class="ltx_Math" alttext="p=0.42" display="inline"><mrow><mi>p</mi><mo>=</mo><mn>0.42</mn></mrow></math> of landing heads. The frequentist definition may be unwieldy however when we consider more unique, one-off situations, e.g., the 2024 US Presidential election. (2) The <a href="https://en.wikipedia.org/wiki/Bayesian_probability" title="" class="ltx_ref ltx_href">Bayesian interpretation</a> is more suitable for such situations, as it considers probability to be one’s degree of belief or opinion that an event will occur. That is, probability is defined as a final representation of one’s state of knowledge about the occurrence of an event, based on a logical assessment of prior observations and evidence. One could imagine a Bayesian believing that if the future of the Universe were split into many equally possible branches, the event would occur in the fraction <math id="S0.SS0.SSSx1.p1.m3" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> of these branches.</p>
</div>
</section>
<section id="S0.SS0.SSSx2" class="ltx_subsubsection">
<h2 class="ltx_title ltx_title_subsubsection">Definition of Surprise</h2>

<div id="S0.SS0.SSSx2.p1" class="ltx_para">
<p class="ltx_p">Now let us define surprise. As you might expect, the term simply measures how surprised we are by the occurrence of an event. If an event with probability <math id="S0.SS0.SSSx2.p1.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> occurs, then our amount of ‘surprise’ is <math id="S0.SS0.SSSx2.p1.m2" class="ltx_Math" alttext="\log(1/p)" display="inline"><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math>. This makes sense if we consider an event that has a 100% probability of occurring. We should have 0 surprise when it occurs, and indeed <math id="S0.SS0.SSSx2.p1.m3" class="ltx_Math" alttext="\log(1/1)=0" display="inline"><mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></math>. Likewise, if an event occurred that we mistakenly believed had probability <math id="S0.SS0.SSSx2.p1.m4" class="ltx_Math" alttext="p=0" display="inline"><mrow><mi>p</mi><mo>=</mo><mn>0</mn></mrow></math> (let us imagine the calculus version of <math id="S0.SS0.SSSx2.p1.m5" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> infinitely approaching 0). Then, our surprise is <math id="S0.SS0.SSSx2.p1.m6" class="ltx_Math" alttext="\log(1/0)=\log(\infty)=\infty" display="inline"><mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mn>0</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi mathvariant="normal">∞</mi></mrow></math>. This is why you should never assign a probability of 0 to any event, because then if it occurs, you will either be infinitely surprised or undefined.</p>
</div>
</section>
<section id="S0.SS0.SSSx3" class="ltx_subsubsection">
<h2 class="ltx_title ltx_title_subsubsection">Kullback-Leibler divergence</h2>

<div id="S0.SS0.SSSx3.p1" class="ltx_para">
<p class="ltx_p">Surprise is closely related to the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" title="" class="ltx_ref ltx_href">Kullback-Leibler (KL) divergence</a>, which is a measure of how different two probability distributions are. To explain this, we must first explain the concept of expected surprise. Let <math id="S0.SS0.SSSx3.p1.m1" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math> be the true probability distribution over observations <math id="S0.SS0.SSSx3.p1.m2" class="ltx_Math" alttext="\textbf{x}\in\mathcal{X}" display="inline"><mrow><mtext>𝐱</mtext><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi></mrow></math> such that <math id="S0.SS0.SSSx3.p1.m3" class="ltx_Math" alttext="p_{\mathrm{true}}(\textbf{x})" display="inline"><mrow><msub><mi>p</mi><mi>true</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow></mrow></math> gives the true probability of observing of <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span>. For our weighted coin example, <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span> <math id="S0.SS0.SSSx3.p1.m6" class="ltx_Math" alttext="=" display="inline"><mo>=</mo></math> the event that the coin lands heads. This event is an element of the set <math id="S0.SS0.SSSx3.p1.m7" class="ltx_Math" alttext="\mathcal{X}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒳</mi></math>, which is the space of all possible events considered by the probability distribution (there are only two – either the coin lands heads or tails). Suppose then that the coin actually lands heads with probability <math id="S0.SS0.SSSx3.p1.m8" class="ltx_Math" alttext="p_{\mathrm{true}}(\textbf{x})" display="inline"><mrow><msub><mi>p</mi><mi>true</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow></mrow></math> <math id="S0.SS0.SSSx3.p1.m9" class="ltx_Math" alttext="=" display="inline"><mo>=</mo></math> 42.690…%. (For the sake of simplicity, I’m intentionally conflating the event space with the domain of a random variable, and the realization of an event with the realization of a random variable.) Let <math id="S0.SS0.SSSx3.p1.m10" class="ltx_Math" alttext="p_{\mathrm{model}}" display="inline"><msub><mi>p</mi><mi>model</mi></msub></math> be our model probability distribution, which is our current guess for <math id="S0.SS0.SSSx3.p1.m11" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math> (we guessed <math id="S0.SS0.SSSx3.p1.m12" class="ltx_Math" alttext="p_{\mathrm{model}}(\textbf{x})" display="inline"><mrow><msub><mi>p</mi><mi>model</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow></mrow></math> = 42.0%).</p>
</div>
<div id="S0.SS0.SSSx3.p2" class="ltx_para">
<p class="ltx_p">For the uninitiated, <math id="S0.SS0.SSSx3.p2.m1" class="ltx_math_unparsed" alttext="\mathbb{E}_{\textbf{x}\sim p_{\mathrm{distr}}}\bigr{[}f(\textbf{x})\bigr{]}" display="inline"><mrow><mrow><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mi>p</mi><mi>distr</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0em">[</mo></mrow><mi>f</mi><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math> is the <a href="https://en.wikipedia.org/wiki/Expected_value" title="" class="ltx_ref ltx_href">expected value</a> of the function <math id="S0.SS0.SSSx3.p2.m2" class="ltx_Math" alttext="f(\textbf{x})" display="inline"><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow></mrow></math> when the events <math id="S0.SS0.SSSx3.p2.m3" class="ltx_Math" alttext="\textbf{x}\in\mathcal{X}" display="inline"><mrow><mtext>𝐱</mtext><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi></mrow></math> occur (i.e. are <em class="ltx_emph ltx_font_italic">distributed</em>) according to the probability distribution <math id="S0.SS0.SSSx3.p2.m4" class="ltx_Math" alttext="p_{\mathrm{distr}}" display="inline"><msub><mi>p</mi><mi>distr</mi></msub></math>. (Expected value and expectation are exactly the same concept as average or mean.) If our function <math id="S0.SS0.SSSx3.p2.m5" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math> is surprise, then the average amount of surprise we experience by using our model is <math id="S0.SS0.SSSx3.p2.m6" class="ltx_math_unparsed" alttext="\mathbb{E}_{\textbf{x}\sim p_{\mathrm{true}}}\bigr{[}\log(1/p_{\mathrm{model}}%
(\textbf{x}))\bigr{]}" display="inline"><mrow><mrow><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>model</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math>, because the data is distributed according to <math id="S0.SS0.SSSx3.p2.m7" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math> and our function is <math id="S0.SS0.SSSx3.p2.m8" class="ltx_Math" alttext="\log(1/p_{\mathrm{model}}(\textbf{x}))" display="inline"><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>model</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></math>. If instead of using an imperfect model, we know the true distribution <math id="S0.SS0.SSSx3.p2.m9" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>, then our expected surprise is <math id="S0.SS0.SSSx3.p2.m10" class="ltx_math_unparsed" alttext="\mathbb{E}_{\textbf{x}\sim p_{\mathrm{true}}}\bigr{[}\log(1/p_{\mathrm{true}}(%
\textbf{x}))\bigr{]}" display="inline"><mrow><mrow><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>true</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math>.</p>
</div>
<div id="S0.SS0.SSSx3.p3" class="ltx_para">
<p class="ltx_p">KL-Divergence is typically understood as a measure of how one probability distribution is different from a second, reference probability distribution. When we let those two probability distributions be <math id="S0.SS0.SSSx3.p3.m1" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math> and <math id="S0.SS0.SSSx3.p3.m2" class="ltx_Math" alttext="p_{\mathrm{model}}" display="inline"><msub><mi>p</mi><mi>model</mi></msub></math>, the equation for KL-Divergence is:</p>
<table id="S0.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S0.Ex1.m1" class="ltx_math_unparsed" alttext="D_{KL}(p_{\mathrm{true}}||p_{\mathrm{model}})=\mathbb{E}_{\textbf{x}\sim p_{%
\mathrm{true}}}\bigr{[}\log(1/p_{\mathrm{model}}(\textbf{x}))\bigr{]}\,-\,%
\mathbb{E}_{\textbf{x}\sim p_{\mathrm{true}}}\bigr{[}\log(1/p_{\mathrm{true}}(%
\textbf{x}))\bigr{]}\ ." display="block"><mrow><mrow><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mo>⁢</mo><mi>L</mi></mrow></msub><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>true</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>p</mi><mi>model</mi></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>model</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%" rspace="0.170em">]</mo></mrow><mo rspace="0.392em">−</mo><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>true</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%" rspace="0.222em">]</mo><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We can see that the KL-divergence to <math id="S0.SS0.SSSx3.p3.m3" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math> from <math id="S0.SS0.SSSx3.p3.m4" class="ltx_Math" alttext="p_{\mathrm{model}}" display="inline"><msub><mi>p</mi><mi>model</mi></msub></math> is the expected excess surprise from using our imperfect model <math id="S0.SS0.SSSx3.p3.m5" class="ltx_Math" alttext="p_{\mathrm{model}}" display="inline"><msub><mi>p</mi><mi>model</mi></msub></math> rather than the ground truth <math id="S0.SS0.SSSx3.p3.m6" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>. That is, the KL-divergence measures the <em class="ltx_emph ltx_font_italic">extra</em> amount of surprise we experience on average when the actual data is distributed according to <math id="S0.SS0.SSSx3.p3.m7" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math> but we are instead working with <math id="S0.SS0.SSSx3.p3.m8" class="ltx_Math" alttext="p_{\mathrm{model}}" display="inline"><msub><mi>p</mi><mi>model</mi></msub></math>. It’s the price we pay – in terms of being surprised – for having the wrong model. It can be shown with <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" title="" class="ltx_ref ltx_href">Jensen’s inequality</a> that the KL-divergence is always positive, except when <math id="S0.SS0.SSSx3.p3.m9" class="ltx_Math" alttext="p_{\mathrm{model}}=p_{\mathrm{true}}" display="inline"><mrow><msub><mi>p</mi><mi>model</mi></msub><mo>=</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></math>, when it is clearly equal to zero. This means that we are always more surprised on average when we are working with the wrong probabilities than when we work with the correct probabilities, which makes intuitive sense.</p>
</div>
</section>
<section id="S0.SS0.SSSx4" class="ltx_subsubsection">
<h2 class="ltx_title ltx_title_subsubsection">Model selection with KL-divergence minimization</h2>

<div id="S0.SS0.SSSx4.p1" class="ltx_para">
<p class="ltx_p">KL-divergence is important in statistics and machine learning because we choose our model by minimizing the KL-divergence. We can now understand this process of model selection as minimizing how surprised we are on average by using our model. Minimizing KL-divergence is also called minimizing the cross-entropy (average surprise is also called <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" title="" class="ltx_ref ltx_href">entropy</a>), and we are taking the <em class="ltx_emph ltx_font_italic">cross</em> (i.e. the difference through subtraction) between our model’s entropy <math id="S0.SS0.SSSx4.p1.m1" class="ltx_math_unparsed" alttext="\mathbb{E}_{\textbf{x}\sim p_{\mathrm{true}}}\bigr{[}\log(1/p_{\mathrm{model}}%
(\textbf{x}))\bigr{]}" display="inline"><mrow><mrow><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>model</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math> and the irreducible ground truth entropy <math id="S0.SS0.SSSx4.p1.m2" class="ltx_math_unparsed" alttext="\mathbb{E}_{\textbf{x}\sim p_{\mathrm{true}}}\bigr{[}\log(1/p_{\mathrm{true}}(%
\textbf{x}))\bigr{]}" display="inline"><mrow><mrow><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>true</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math>. Minimizing the KL-divergence is also equivalent to finding the <a href="https://en.wikipedia.org/wiki/Bayes_estimator" title="" class="ltx_ref ltx_href">Bayes estimator</a> when our cost function is surprise. One could imagine that we are pained by surprise and therefore want to expect as little of the emotion as possible, however we have no bias towards the kind of surprise we experience. That is, we do not have a preference for or against being surprised by making a false positive or a false negative (for classification tasks), or by overestimating or underestimating (for regression tasks).</p>
</div>
<div id="S0.SS0.SSSx4.p2" class="ltx_para">
<p class="ltx_p">This concept of surprise tends to favour the Bayesian definition of probability. Surprise can be viewed as the emotion one experiences when one’s beliefs <math id="S0.SS0.SSSx4.p2.m1" class="ltx_Math" alttext="p_{\mathrm{model}}" display="inline"><msub><mi>p</mi><mi>model</mi></msub></math> do not match reality <math id="S0.SS0.SSSx4.p2.m2" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>. We see however that even when our beliefs are correct (<math id="S0.SS0.SSSx4.p2.m3" class="ltx_Math" alttext="p_{\mathrm{model}}=p_{\mathrm{true}}" display="inline"><mrow><msub><mi>p</mi><mi>model</mi></msub><mo>=</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></math>), we still experience an irreducible amount of surprise on average (i.e. entropy) given by <math id="S0.SS0.SSSx4.p2.m4" class="ltx_math_unparsed" alttext="\mathbb{E}_{\textbf{x}\sim p_{\mathrm{true}}}\bigr{[}\log(1/p_{\mathrm{true}}(%
\textbf{x}))\bigr{]}" display="inline"><mrow><mrow><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>true</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math>, we cannot get any less surprised than this. The irreducibility of this surprise begs the question of what we mean by <math id="S0.SS0.SSSx4.p2.m5" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>. We said earlier that if we knew everything about the Universe, then every observed event has probability 1. That is, the omniscient observer is never surprised. We consider the existence of <math id="S0.SS0.SSSx4.p2.m6" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math> only because we imagine there is some level of irreducible uncertainty in the universe.</p>
</div>
<div id="S0.SS0.SSSx4.p3" class="ltx_para">
<p class="ltx_p">For example, suppose we want to predict some target <math id="S0.SS0.SSSx4.p3.m1" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> (e.g. the height of a child) based on known observations <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span> (e.g. the heights of the child’s parents). Then <math id="S0.SS0.SSSx4.p3.m3" class="ltx_Math" alttext="p(y|\textbf{x})" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo fence="false">|</mo><mtext>𝐱</mtext></mrow><mo stretchy="false">)</mo></mrow></mrow></math> gives the <a href="https://en.wikipedia.org/wiki/Conditional_probability" title="" class="ltx_ref ltx_href">conditional probability</a> of the target <math id="S0.SS0.SSSx4.p3.m4" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> based on the observations <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span>. This mapping from our observations <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span> to our target <math id="S0.SS0.SSSx4.p3.m7" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> may be inherently stochastic. That is, there may be some completely unpredictable biological randomness in the determination of height. Or, perhaps <math id="S0.SS0.SSSx4.p3.m8" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> <em class="ltx_emph ltx_font_italic">is</em> deterministic but depends on other variables not included in <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span>, which we therefore do not observe, e.g., the specific alleles of the parents and the diet of the child. Whether the system is either inherently stochastic or we do not have all the necessary information, the ground truth <math id="S0.SS0.SSSx4.p3.m10" class="ltx_Math" alttext="p_{\mathrm{true}}(y|\textbf{x})" display="inline"><mrow><msub><mi>p</mi><mi>true</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo fence="false">|</mo><mtext>𝐱</mtext></mrow><mo stretchy="false">)</mo></mrow></mrow></math> is in either case entropic (i.e. surprising). The lowest error we could possibly hope to obtain by our model, the error we would incur if we were an oracle making predictions from this ‘true’ distribution, is known as the <a href="https://en.wikipedia.org/wiki/Bayes_error_rate" title="" class="ltx_ref ltx_href">Bayes error rate</a>. That is, with surprise as our cost function, the Bayes error rate is the irreducible average surprise given by <math id="S0.SS0.SSSx4.p3.m11" class="ltx_math_unparsed" alttext="\mathbb{E}_{\textbf{x}\sim p_{\mathrm{true}}}\bigr{[}\log(1/p_{\mathrm{true}}(%
\textbf{x}))\bigr{]}" display="inline"><mrow><mrow><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mi>p</mi><mi>true</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>true</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math>.</p>
</div>
</section>
<section id="S0.SS0.SSSx5" class="ltx_subsubsection">
<h2 class="ltx_title ltx_title_subsubsection">KL-divergence minimization is maximum likelihood estimation</h2>

<div id="S0.SS0.SSSx5.p1" class="ltx_para">
<p class="ltx_p">If we were to try to minimize the KL-divergence of our model in the way just described, we would need access to <math id="S0.SS0.SSSx5.p1.m1" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>. Unfortunately, this is almost never the case (if we knew <math id="S0.SS0.SSSx5.p1.m2" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>, then there would be no need to build <math id="S0.SS0.SSSx5.p1.m3" class="ltx_Math" alttext="p_{\mathrm{model}}" display="inline"><msub><mi>p</mi><mi>model</mi></msub></math>). As an approximation to <math id="S0.SS0.SSSx5.p1.m4" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>, we use <math id="S0.SS0.SSSx5.p1.m5" class="ltx_Math" alttext="\hat{p}_{\mathrm{data}}" display="inline"><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>data</mi></msub></math>, which is simply the dataset, i.e., the empirical distribution of the observations. It consists of paired observations <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span> and <math id="S0.SS0.SSSx5.p1.m7" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math>. This is where the term <em class="ltx_emph ltx_font_italic">training</em> comes from in machine learning, as we typically split the dataset randomly into a set for <em class="ltx_emph ltx_font_italic">training</em> the model (showing it observations of <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span> paired with <math id="S0.SS0.SSSx5.p1.m9" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math>) and a separate set for <em class="ltx_emph ltx_font_italic">testing</em> the performance of model (showing it only <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span> and asking it to predict <math id="S0.SS0.SSSx5.p1.m11" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math>). We hope that the model, which is like a ‘machine’, <em class="ltx_emph ltx_font_italic">learns</em> effectively from the training set, hence we call it machine learning. Admittedly, the terminology is a bit pretentious. By using this method, we are hoping that the empirical distribution of the data <math id="S0.SS0.SSSx5.p1.m12" class="ltx_Math" alttext="\hat{p}_{\mathrm{data}}" display="inline"><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>data</mi></msub></math> matches <math id="S0.SS0.SSSx5.p1.m13" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>, which is more likely to occur in large diverse datasets that converge with fidelity to the population distribution of possible events. We also hope that there is sufficient dependency between our target <math id="S0.SS0.SSSx5.p1.m14" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> and the information provided within each observation <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span> such that our target <math id="S0.SS0.SSSx5.p1.m16" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> <em class="ltx_emph ltx_font_italic">could</em> in theory be predicted from <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span>.</p>
</div>
<div id="S0.SS0.SSSx5.p2" class="ltx_para">
<p class="ltx_p">As a way of selecting the optimal <math id="S0.SS0.SSSx5.p2.m1" class="ltx_Math" alttext="p_{\mathrm{model}}" display="inline"><msub><mi>p</mi><mi>model</mi></msub></math> (the one that minimizes the KL-divergence) it is often most convenient to utilize a parametric family of probability distributions <math id="S0.SS0.SSSx5.p2.m2" class="ltx_Math" alttext="p_{\mathrm{model}}(\textbf{x},\boldsymbol{\theta})" display="inline"><mrow><msub><mi>p</mi><mi>model</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></math>, which defines a space of possible models indexed by the parameter <math id="S0.SS0.SSSx5.p2.m3" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math> (note that <math id="S0.SS0.SSSx5.p2.m4" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math> can be an array of multiple constituent parameters). We then search for the optimal model within this space of models by searching for the best parameter <math id="S0.SS0.SSSx5.p2.m5" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math>. By optimal model, we mean the one that gets as close as possible to the elusive <math id="S0.SS0.SSSx5.p2.m6" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>. The KL-divergence minimization is therefore performed by adjusting the parameter <math id="S0.SS0.SSSx5.p2.m7" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math>. In practice, we must use <math id="S0.SS0.SSSx5.p2.m8" class="ltx_Math" alttext="\hat{p}_{\mathrm{data}}" display="inline"><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>data</mi></msub></math> in place of <math id="S0.SS0.SSSx5.p2.m9" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math>, in which case our KL-divergence is</p>
<table id="S0.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S0.Ex2.m1" class="ltx_math_unparsed" alttext="D_{KL}(p_{\mathrm{true}}||p_{\mathrm{model}})=\mathbb{E}_{\textbf{x}\sim\hat{p%
}_{\mathrm{data}}}\bigr{[}\log(1/p_{\mathrm{model}}(\textbf{x};\boldsymbol{%
\theta}))\bigr{]}\,-\,\mathbb{E}_{\textbf{x}\sim\hat{p}_{\mathrm{data}}}\bigr{%
[}\log(1/\hat{p}_{\mathrm{data}}(\textbf{x}))\bigr{]}" display="block"><mrow><mrow><mrow><mrow><msub><mi>D</mi><mrow><mi>K</mi><mo>⁢</mo><mi>L</mi></mrow></msub><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>true</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>p</mi><mi>model</mi></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>data</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>model</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo>;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%" rspace="0.170em">]</mo></mrow><mo rspace="0.392em">−</mo><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>data</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>data</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We can neglect the term on the right in the minimization procedure because it is simply a function of the data, not of <math id="S0.SS0.SSSx5.p2.m10" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math>. We therefore simply need to find the parameter <math id="S0.SS0.SSSx5.p2.m11" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math> that minimizes
</p>
<table id="S0.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S0.Ex3.m1" class="ltx_math_unparsed" alttext="\mathbb{E}_{\textbf{x}\sim\hat{p}_{\mathrm{data}}}\bigr{[}\log(1/p_{\mathrm{%
model}}(\textbf{x};\boldsymbol{\theta}))\bigr{]}" display="block"><mrow><mrow><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>data</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>model</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo>;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S0.SS0.SSSx5.p3" class="ltx_para">
<p class="ltx_p">It might surprise you that this minimization is identical to <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" title="" class="ltx_ref ltx_href">maximum likelihood estimation (MLE)</a>. We can see this connection if we look at the definition of the maximum likelihood estimator for <math id="S0.SS0.SSSx5.p3.m1" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math> (for <math id="S0.SS0.SSSx5.p3.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> observations <math id="S0.SS0.SSSx5.p3.m3" class="ltx_Math" alttext="\textbf{x}^{(i)}" display="inline"><msup><mtext>𝐱</mtext><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></math> in the data):</p>
<table id="S0.Ex4" class="ltx_equationgroup ltx_eqn_table">

<tbody id="S0.Ex4X"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S0.Ex4X.m2" class="ltx_Math" alttext="\displaystyle\boldsymbol{\theta}_{\mathrm{MLE}}{}" display="inline"><msub><mi>𝜽</mi><mi>MLE</mi></msub></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S0.Ex4X.m3" class="ltx_Math" alttext="\displaystyle=\operatorname*{argmax}_{\boldsymbol{\theta}}\,p_{\mathrm{model}}%
(\text{data};\boldsymbol{\theta})" display="inline"><mrow><mi></mi><mo rspace="0.1389em">=</mo><mrow><mrow><munder><mo lspace="0.1389em" rspace="0.167em">argmax</mo><mi>𝜽</mi></munder><msub><mi>p</mi><mi>model</mi></msub></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mtext>data</mtext><mo>;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S0.Ex4Xa"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S0.Ex4Xa.m2" class="ltx_Math" alttext="\displaystyle=\operatorname*{argmax}_{\boldsymbol{\theta}}\,\log(\prod_{i=1}^{%
n}p_{\mathrm{model}}(\textbf{x}^{(i)};\boldsymbol{\theta}))" display="inline"><mrow><mi></mi><mo rspace="0.1389em">=</mo><mrow><mrow><munder><mo lspace="0.1389em" rspace="0.167em">argmax</mo><mi>𝜽</mi></munder><mi>log</mi></mrow><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><msub><mi>p</mi><mi>model</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mtext>𝐱</mtext><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S0.Ex4Xb"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S0.Ex4Xb.m2" class="ltx_Math" alttext="\displaystyle=\operatorname*{argmax}_{\boldsymbol{\theta}}\,\sum_{i=1}^{n}\log%
(p_{\mathrm{model}}(\textbf{x}^{(i)};\boldsymbol{\theta}))" display="inline"><mrow><mi></mi><mo rspace="0.1389em">=</mo><mrow><munder><mo lspace="0.1389em">argmax</mo><mi>𝜽</mi></munder><mo lspace="0.167em">⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>p</mi><mi>model</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msup><mtext>𝐱</mtext><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S0.Ex4Xc"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S0.Ex4Xc.m2" class="ltx_math_unparsed" alttext="\displaystyle=\operatorname*{argmax}_{\boldsymbol{\theta}}\,\mathbb{E}_{%
\textbf{x}\sim\hat{p}_{\mathrm{data}}}\bigr{[}\log(p_{\mathrm{model}}(\textbf{%
x};\boldsymbol{\theta}))\bigr{]}" display="inline"><mrow><mrow><mo rspace="0.1389em">=</mo><munder><mo lspace="0.1389em" rspace="0.167em">argmax</mo><mi>𝜽</mi></munder><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>data</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><msub><mi>p</mi><mi>model</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo>;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%">]</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S0.Ex4Xd"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S0.Ex4Xd.m2" class="ltx_math_unparsed" alttext="\displaystyle=\operatorname*{argmin}_{\boldsymbol{\theta}}\,\mathbb{E}_{%
\textbf{x}\sim\hat{p}_{\mathrm{data}}}\bigr{[}\log(1/p_{\mathrm{model}}(%
\textbf{x};\boldsymbol{\theta}))\bigr{]}\ ." display="inline"><mrow><mrow><mo rspace="0.1389em">=</mo><munder><mo lspace="0.1389em" rspace="0.167em">argmin</mo><mi>𝜽</mi></munder><msub><mi>𝔼</mi><mrow><mtext>𝐱</mtext><mo>∼</mo><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>data</mi></msub></mrow></msub><mo lspace="0em" mathsize="120%" rspace="0.167em">[</mo></mrow><mi>log</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msub><mi>p</mi><mi>model</mi></msub><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo>;</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo maxsize="120%" minsize="120%" rspace="0.222em">]</mo><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">We typically think of MLE as picking the parameter <math id="S0.SS0.SSSx5.p3.m4" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math> that maximizes the probability of observing the dataset, under the assumption that the data points are approximately independent and identically distributed according to <math id="S0.SS0.SSSx5.p3.m5" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math> and <math id="S0.SS0.SSSx5.p3.m6" class="ltx_Math" alttext="p_{\mathrm{true}}" display="inline"><msub><mi>p</mi><mi>true</mi></msub></math> is within the parametric family <math id="S0.SS0.SSSx5.p3.m7" class="ltx_Math" alttext="p_{\mathrm{model}}(\textbf{x},\boldsymbol{\theta})" display="inline"><mrow><msub><mi>p</mi><mi>model</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mtext>𝐱</mtext><mo>,</mo><mi>𝜽</mi><mo stretchy="false">)</mo></mrow></mrow></math>. Now, we see that this method is equivalent to minimizing the expected surprise, under this same assumption.</p>
</div>
</section>
<section id="S0.SS0.SSSx6" class="ltx_subsubsection">
<h2 class="ltx_title ltx_title_subsubsection">Linear regression is Gaussian surprise minimization</h2>

<div id="S0.SS0.SSSx6.p1" class="ltx_para">
<p class="ltx_p">If you’ve ever used linear regression, you’ve performed MLE. Linear regression makes predictions with a linear combination of observed features <span class="ltx_text ltx_markedasmath ltx_font_bold">x</span>, weighted by <math id="S0.SS0.SSSx6.p1.m2" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math>, and penalizes the sum of the squared errors between the predictions and the observed data. In this way, the model obtains the hyperplane of best fit in the space <math id="S0.SS0.SSSx6.p1.m3" class="ltx_Math" alttext="\mathcal{X}\cup\mathbb{R}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo>∪</mo><mi>ℝ</mi></mrow></math> (where <math id="S0.SS0.SSSx6.p1.m4" class="ltx_Math" alttext="\textbf{x}\in\mathcal{X}" display="inline"><mrow><mtext>𝐱</mtext><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi></mrow></math> and <math id="S0.SS0.SSSx6.p1.m5" class="ltx_Math" alttext="y\in\mathbb{R}" display="inline"><mrow><mi>y</mi><mo>∈</mo><mi>ℝ</mi></mrow></math>). Penalizing the squared error is equivalent to assuming a Gaussian parametric model <math id="S0.SS0.SSSx6.p1.m6" class="ltx_Math" alttext="p_{\mathrm{model}}(y|\textbf{x},\boldsymbol{\theta})=\mathcal{N}(\boldsymbol{%
\theta}^{T}\textbf{x},1)" display="inline"><mrow><mrow><msub><mi>p</mi><mi>model</mi></msub><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo fence="false">|</mo><mrow><mtext>𝐱</mtext><mo>,</mo><mi>𝜽</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>𝜽</mi><mi>T</mi></msup><mo>⁢</mo><mtext>𝐱</mtext></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></math> in which the data is assumed to be normally distributed (with irrelevant variance for making predictions) about the prediction <math id="S0.SS0.SSSx6.p1.m7" class="ltx_Math" alttext="\boldsymbol{\theta}^{T}\textbf{x}" display="inline"><mrow><msup><mi>𝜽</mi><mi>T</mi></msup><mo>⁢</mo><mtext>𝐱</mtext></mrow></math>. The equivalency between the squared error cost and the Gaussian parametric family arises simply because the <a href="https://en.wikipedia.org/wiki/Normal_distribution" title="" class="ltx_ref ltx_href">Gaussian probability density function</a> actually contains the squared difference between the prediction <math id="S0.SS0.SSSx6.p1.m8" class="ltx_Math" alttext="\boldsymbol{\theta}^{T}\textbf{x}" display="inline"><mrow><msup><mi>𝜽</mi><mi>T</mi></msup><mo>⁢</mo><mtext>𝐱</mtext></mrow></math> and the observed target <math id="S0.SS0.SSSx6.p1.m9" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math>: <math id="S0.SS0.SSSx6.p1.m10" class="ltx_Math" alttext="(y\,-\,\boldsymbol{\theta}^{T}\textbf{x})^{2}" display="inline"><msup><mrow><mo stretchy="false">(</mo><mrow><mi>y</mi><mo lspace="0.392em" rspace="0.392em">−</mo><mrow><msup><mi>𝜽</mi><mi>T</mi></msup><mo>⁢</mo><mtext>𝐱</mtext></mrow></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></math> in an exponential and that exponential is stripped away when the <math id="S0.SS0.SSSx6.p1.m11" class="ltx_Math" alttext="\log" display="inline"><mi>log</mi></math> is applied in the <math id="S0.SS0.SSSx6.p1.m12" class="ltx_Math" alttext="\boldsymbol{\theta}_{\mathrm{MLE}}" display="inline"><msub><mi>𝜽</mi><mi>MLE</mi></msub></math> summation (as shown above). Likewise, the product of the joint probability of independent and identically distributed observations becomes the sum of the terms in the exponential (the squared error), as the <math id="S0.SS0.SSSx6.p1.m13" class="ltx_Math" alttext="\log" display="inline"><mi>log</mi></math> operation turns a product <math id="S0.SS0.SSSx6.p1.m14" class="ltx_Math" alttext="\Pi" display="inline"><mi mathvariant="normal">Π</mi></math> to a sum <math id="S0.SS0.SSSx6.p1.m15" class="ltx_Math" alttext="\Sigma" display="inline"><mi mathvariant="normal">Σ</mi></math> (also as shown above). This is why MLE with the Gaussian parametric family is equivalent to minimizing the sum of the squared errors, i.e., the method of least squares. Therefore, we see that using the squared error as the cost function is identical to using surprise as the cost function and assuming the data is Gaussian distributed (both approaches have the same Bayes estimator).</p>
</div>
</section>
<section id="S0.SS0.SSSx7" class="ltx_subsubsection">
<h2 class="ltx_title ltx_title_subsubsection">Surprise is also information content</h2>

<div id="S0.SS0.SSSx7.p1" class="ltx_para">
<p class="ltx_p">It should also be noted that surprise is also known as <a href="https://en.wikipedia.org/wiki/Information_content" title="" class="ltx_ref ltx_href">information content</a> (or Shannon information). This makes sense when we consider the obvious fact that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. In the extreme case, when we learn that an event occurred that we believed had a 100% chance of occurring, we are clearly provided with with no new information. We often use base-2 for the logarithm in <math id="S0.SS0.SSSx7.p1.m1" class="ltx_Math" alttext="\log(1/p)" display="inline"><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>p</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math>, so that the amount of information provided by an event can be measured in bits (an event that has probability <math id="S0.SS0.SSSx7.p1.m2" class="ltx_Math" alttext="p=1/2" display="inline"><mrow><mi>p</mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></math> provides one bit of information when it occurs). In this view, the entropy of a probability distribution tells us the average amount of information provided by an event drawn from that distribution. Equivalently, the entropy gives us the number of bits needed on average to encode events drawn from that distribution, e.g., if we were communicating the outcome of the events over a communication channel and wanted to use as few bits as possible.</p>
</div>
</section>
<section id="S0.SS0.SSSx8" class="ltx_subsubsection">
<h2 class="ltx_title ltx_title_subsubsection">Surprise in everyday life</h2>

<div id="S0.SS0.SSSx8.p1" class="ltx_para">
<p class="ltx_p">This is getting a bit rambly. There’s more I want to say about surprise so I should continue this in a second blog post. The main reason I thought surprise was interesting is that I think it is a connection between statistics and the human emotional experience. It is the feeling of surprise that repeatedly teaches us the hard lesson that the world indeed is probabilistic, not deterministic, and that we cannot predict the future perfectly. By understanding ideas of probability and statistics in terms of surprise, we can see these same ideas from the same intuitive perspective we use for our every-day understanding of the world and hopefully then gain a deeper intuitive understanding of statistics.</p>
</div>
<div id="S0.SS0.SSSx8.p2" class="ltx_para">
<p class="ltx_p">Instead of thinking of modeling as maximizing likelihood, whatever that means, we can think of modeling as a way of trying to be less surprised by events. We make our model of the world under a surprise-minimization framework, which tends to be how we go through our lives anyways. Perhaps this is part of our brains’ vestigial survival mechanism. Our <a href="https://en.wikipedia.org/wiki/Ecological_niche" title="" class="ltx_ref ltx_href">ecological niche</a> was having sophisticated brains that could form a high-fidelity model of the complex world around us and act on those models, finding ingenious and resourceful paths to evolutionary success. Even if our model was technically wrong, as long as it was useful, it was good. This goes with the well-known saying in statistics: <a href="https://en.wikipedia.org/wiki/All_models_are_wrong" title="" class="ltx_ref ltx_href">All models are wrong, but some are useful</a>. Much of our model of the world exists within intuition and common sense: that <a href="https://en.wikipedia.org/wiki/Unconscious_mind" title="" class="ltx_ref ltx_href">ineffable knowledge-base</a> that shapes our actions through some automatic, neigh latent mechanism. This model can be updated by the emotion of surprise and its associated valence, depending on the event that elicits it. On the other hand, when someone makes the sly comment in response to some piece of news ‘I’m not surprised,’ often what they are really saying is that their mental model of the world had placed a relatively high degree of belief or probability in the event possibly occurring and they may even feel some pride in the fidelity of their mental model.</p>
</div>
<div id="S0.SS0.SSSx8.p3" class="ltx_para">
<p class="ltx_p">There is a tendency among humans to perceive past events as having been more predictable than they actually were. This is known as <a href="https://en.wikipedia.org/wiki/Hindsight_bias" title="" class="ltx_ref ltx_href">hindsight bias</a>. Perhaps we have this bias because surprise promotes a sense-making process in which we change our model of the world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="The role of surprise in hindsight bias: a metacognitive model of reduced and reversed hindsight bias" class="ltx_ref">3</a>]</cite>. Under our new updated model of the world, the event is more likely and we mistakenly misremember that we held this new model of the world all along. This might also explain why hindsight bias has been found to be more likely to occur when the outcome of an event is negative rather than positive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="Expectation-outcome consistency and hindsight bias" class="ltx_ref">4</a>]</cite>. We are more sensitive to negative events and our model of the world is updated more in response to the surprise of a negative event. That is, in our belief and expectation-forming process, we do not perform simple maximum likelihood estimation, because we change our model more in response to negative events than in response to positive events. None of us likes to be surprised by negative events, we do not like <a href="https://en.wikipedia.org/wiki/Black_swan_theory" title="" class="ltx_ref ltx_href">black swans</a>. However being surprised by positive events can also be disorienting as it makes us realize that we had the wrong model of the world. We are confronted with the missed opportunities by having an overly pessimistic or conservative outlook. Now we can see the inherent cost function of our inner model: We penalize surprise, especially of negative events. Quoting the English journalist Walter Bagehot: “One of the greatest pains to human nature is the pain of a new idea.”</p>
</div>
<div id="S0.SS0.SSSx8.p4" class="ltx_para">
<p class="ltx_p">Since the surprise of an event is equivalent to the amount of information provided by it, we should be unsurprised (no pun intended) that modern information-providing outlets, that cater to the demands of our anachronistic brains, i.e. the news and media, tend to publicize unlikely events. Furthermore, to best update our Bayesian belief systems (models) about the world, we would expect these outlets to publicize the kinds of events that us humans are most sensitive to: negative events. Unsurprisingly as well, there is substantial research to show that the news tends to be negative, explaining the adage in media “if it bleeds, it leads” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="Cross-national evidence of a negativity bias in psychophysiological reactions to news" class="ltx_ref">5</a>]</cite>. As you read this now, please open up your favorite news website of choice, and I will bet you that within the top stories, there will be one about an event that is both relatively unlikely <em class="ltx_emph ltx_font_italic">and</em> negative. If I am wrong then please send me a contemporaneous screenshot along with your bank details and I will transfer you £0.42. I should note that I got some of the ideas in this post from Joseph Blitzstein’s <a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view" title="" class="ltx_ref ltx_href">Probability textbook</a> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="Introduction to probability" class="ltx_ref">1</a>]</cite> and Ian Goodfellow’s <a href="https://www.deeplearningbook.org/" title="" class="ltx_ref ltx_href">Deep Learning textbook</a> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="Deep learning" class="ltx_ref">2</a>]</cite>. I highly recommend giving both books a read.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib2" class="ltx_bibitem ltx_bib_book">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J.K. Blitzstein and J. Hwang</span><span class="ltx_text ltx_bib_year"> (2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Introduction to probability</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Chapman &amp; Hall/CRC Texts in Statistical Science</span>,  <span class="ltx_text ltx_bib_publisher">CRC Press</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 9781466575592</span>,
<span class="ltx_text lccn ltx_bib_external">LCCN 2014022709</span>,
<a href="https://books.google.co.uk/books?id=z2POBQAAQBAJ" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_book">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">I. Goodfellow, Y. Bengio, and A. Courville</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep learning</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">MIT Press</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.deeplearningbook.org" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. A. Müller and D. Stahlberg</span><span class="ltx_text ltx_bib_year"> (2007-02)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The role of surprise in hindsight bias: a metacognitive model of reduced and reversed hindsight bias</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Social Cognition</span> <span class="ltx_text ltx_bib_volume">25</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 165–184</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://dx.doi.org/10.1521/soco.2007.25.1.165" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="https://doi.org/10.1521/soco.2007.25.1.165" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. A. Schkade and L. M. Kilbourne</span><span class="ltx_text ltx_bib_year"> (1991-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Expectation-outcome consistency and hindsight bias</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Organizational Behavior and Human Decision Processes</span> <span class="ltx_text ltx_bib_volume">49</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 105–123</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://dx.doi.org/10.1016/0749-5978%2891%2990044-t" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="https://doi.org/10.1016/0749-5978(91)90044-t" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Soroka, P. Fournier, and L. Nir</span><span class="ltx_text ltx_bib_year"> (2019-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cross-national evidence of a negativity bias in psychophysiological reactions to news</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Proceedings of the National Academy of Sciences</span> <span class="ltx_text ltx_bib_volume">116</span> (<span class="ltx_text ltx_bib_number">38</span>), <span class="ltx_text ltx_bib_pages"> pp. 18888–18892</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://dx.doi.org/10.1073/pnas.1908369116" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="https://doi.org/10.1073/pnas.1908369116" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
</div></footer>
</div>
</body>
</html>
